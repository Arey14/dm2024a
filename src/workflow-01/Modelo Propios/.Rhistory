source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/ambiente/z301_GrabarMisSemillas.r")
fwrite( tabla_semillas,
file = "~/Desktop/ITBA/Mineria de Datos/dm2024a/src/workflow-01/Modelo Propios/buckets/b1/datasets/mis_semillas.txt",
sep = "\t"
)
require( "data.table" )
# reemplazar aqui por SUS semillas
mis_semillas <- c(734471, 734473, 734477, 734479, 734497, 734537, 734543, 734549, 734557, 734567, 734627,734647,734653,734659, 734663, 734687, 734693, 734707, 734717, 734729)
tabla_semillas <- as.data.table(list( semilla = mis_semillas ))
fwrite( tabla_semillas,
file = "~/Desktop/ITBA/Mineria de Datos/dm2024a/src/workflow-01/Modelo Propios/buckets/b1/datasets/mis_semillas.txt",
sep = "\t"
)
View(tabla_semillas)
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/ambiente/z301_GrabarMisSemillas.r")
HT_tuning_base <- function( pinputexps, bypass=FALSE)
{
if( -1 == (param_local <- exp_init(pbypass=bypass))$resultado ) return( 0 ) # linea fija bypass
param_local$meta$script <- "/src/workflow-01/z581_HT_lightgbm.r"
# En caso que se haga cross validation, se usa esta cantidad de folds
param_local$lgb_crossvalidation_folds <- 5
param_local$train$clase01_valor1 <- c( "BAJA+2", "BAJA+1")
param_local$train$positivos <- c( "BAJA+2")
param_local$train$gan1 <- 117000
param_local$train$gan0 <-  -3000
param_local$train$meseta <- 2001
# Hiperparametros  del LightGBM
#  los que tienen un solo valor son los que van fijos
#  los que tienen un vector,  son los que participan de la Bayesian Optimization
param_local$lgb_param <- list(
boosting = "gbdt", # puede ir  dart  , ni pruebe random_forest
objective = "binary",
metric = "custom",
first_metric_only = TRUE,
boost_from_average = TRUE,
feature_pre_filter = FALSE,
force_row_wise = TRUE, # para reducir warnings
verbosity = -100,
max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo
min_gain_to_split = 0.0, # min_gain_to_split >= 0.0
min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0
lambda_l1 = 0.0, # lambda_l1 >= 0.0
lambda_l2 = 0.0, # lambda_l2 >= 0.0
max_bin = 31L, # lo debo dejar fijo, no participa de la BO
num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds
bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0
pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0
neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0
is_unbalance = FALSE, #
scale_pos_weight = 1.0, # scale_pos_weight > 0.0
drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0
max_drop = 50, # <=0 means no limit
skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0
extra_trees = FALSE,
# Parte variable
learning_rate = c( 0.02, 0.8 ),
feature_fraction = c( 0.5, 0.9 ),
num_leaves = c( 8L, 2048L,  "integer" ),
min_data_in_leaf = c( 100L, 10000L, "integer" )
)
# una Bayesian humilde, pero no descabellada
param_local$bo_iteraciones <- 60 # iteraciones de la Optimizacion Bayesiana
return( exp_correr_script( param_local ) ) # linea fija
}
