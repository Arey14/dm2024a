xlab("Altura (en cm)") +
ylab("Peso (en kg)")
summary(reg_alturas)
n = nrow(medidas)
set.seed(1234)
muestras <- 1:n %>% createDataPartition(p = 0.8 , list = FALSE)
conjunto_entrenamiento <- medidas[muestras, ]
conjunto_prueba <- medidas[-muestras, ]
porcentaje_entrenamiento <- nrow(conjunto_entrenamiento) / n * 100
porcentaje_prueba <- nrow(conjunto_prueba) / n * 100
cat("Porcentaje de datos en el conjunto de entrenamiento:", porcentaje_entrenamiento, "%\n")
cat("Porcentaje de datos en el conjunto de prueba:", porcentaje_prueba, "%\n")
reg_medidas <- lm(formula = Peso ~ Biacromial + Ancho.pélvico + Bitrocantérico  + Profundidad.de.pecho  + Diámetro.de.pecho + Diámetro.de.codo + Diámetro.de.muñeca + Diámetro.de.rodilla + Diámetro.de.tobillo + Contorno.de.hombro + Contorno.de.pecho  + Contorno.de.cintura + Contorno.de.ombligo + Contorno.de.cadera  + Contorno.de.muslo + Contorno.de.bíceps  + Contorno.de.antebrazo + Contorno.de.rodilla + Contorno.de.pantorrilla + Contorno.de.tobillo + Contorno.de.muñeca  + Edad + Altura, data = conjunto_entrenamiento)
round(reg_medidas$coefficients,3)
reg_medidas2 <- lm(formula = Peso ~   Profundidad.de.pecho + Diámetro.de.rodilla   + Contorno.de.pecho  + Contorno.de.cintura  + Contorno.de.cadera  + Contorno.de.muslo  + Contorno.de.antebrazo + Contorno.de.rodilla + Contorno.de.pantorrilla + Edad + Altura, data = conjunto_entrenamiento)
summary(reg_medidas)
summary(reg_medidas2)
round(reg_medidas2$coefficients,3)
conjunto_prueba$modelo = (-116.745 +
-0.10649 * conjunto_prueba$Biacromial +
0.13638 * conjunto_prueba$Ancho.pélvico +
-0.05228 * conjunto_prueba$Bitrocantérico +
0.29965 * conjunto_prueba$Profundidad.de.pecho +
0.14243 * conjunto_prueba$Diámetro.de.pecho +
0.14037 * conjunto_prueba$Diámetro.de.codo +
0.30880 * conjunto_prueba$Diámetro.de.muñeca +
0.36165 * conjunto_prueba$Diámetro.de.rodilla +
0.29943 * conjunto_prueba$Diámetro.de.tobillo +
0.05855 * conjunto_prueba$Contorno.de.hombro +
0.18343 * conjunto_prueba$Contorno.de.pecho +
0.31672 * conjunto_prueba$Contorno.de.cintura +
0.01888 * conjunto_prueba$Contorno.de.ombligo +
0.26093 * conjunto_prueba$Contorno.de.cadera +
0.23083 * conjunto_prueba$Contorno.de.muslo +
0.02407 * conjunto_prueba$Contorno.de.bíceps +
0.43032 * conjunto_prueba$Contorno.de.antebrazo +
0.27090 * conjunto_prueba$Contorno.de.rodilla +
0.35148 * conjunto_prueba$Contorno.de.pantorrilla +
-0.06504 * conjunto_prueba$Contorno.de.tobillo +
-0.15169 * conjunto_prueba$Contorno.de.muñeca +
-0.08307 * conjunto_prueba$Edad +
0.27063 * conjunto_prueba$Altura)
y_hat_modelo2 <- function(pp, dr, cp,cc,cca,cm,ca,cr,cpa,ed,al){
return(-117.860 + 0.271 * pp + 0.561 * dr + 0.258 * cp + 0.329 * cc + 0.290*cca + 0.229*cm +0.480*ca +0.233*cr+ 0.346*cpa -0.072*ed +0.290*al)
}
conjunto_prueba$modelo2 = (-117.860 + 0.271 * conjunto_prueba$Profundidad.de.pecho + 0.561 * conjunto_prueba$Diámetro.de.rodilla + 0.258 *  conjunto_prueba$Contorno.de.pecho + 0.329 * conjunto_prueba$Contorno.de.cintura + 0.290*conjunto_prueba$Contorno.de.cadera + 0.229* conjunto_prueba$Contorno.de.muslo +0.480* conjunto_prueba$Contorno.de.antebrazo +0.233*conjunto_prueba$Contorno.de.rodilla+ 0.346*conjunto_prueba$Contorno.de.pantorrilla -0.072*conjunto_prueba$Edad +0.290*conjunto_prueba$Altura)
conjunto_prueba
sum((conjunto_prueba$Peso - conjunto_prueba$modelo)^2)/nrow(conjunto_prueba)
sum((conjunto_prueba$Peso - conjunto_prueba$modelo2)^2)/nrow(conjunto_prueba)
library(Metrics)
mse(conjunto_prueba$Peso,conjunto_prueba$modelo)
mse(conjunto_prueba$Peso,conjunto_prueba$modelo2)
dolor = read.xlsx("Dolor.xlsx")
dolor = na.omit(dolor)
ggplot(data = dolor, aes(x = Colesterol, y = Estrechamiento.arterias.coronarias)) +
geom_point(color = "pink") +
geom_smooth(method = "glm", method.args = list(family = "binomial"), color = "purple") +
xlab("Colesterol") +
ylab("Estrechamiento arterias coronarias")
theme()
log_simple <- glm(formula = Estrechamiento.arterias.coronarias ~ Colesterol,
data = dolor,
family = "binomial")
summary(log_simple)
nuevo_valor_colesterol <- data.frame(Colesterol = c(199))
prob_Arterial <- log_simple %>% predict(nuevo_valor_colesterol, type = "response")
prob_Arterial
log_mult <- glm(formula =  Estrechamiento.arterias.coronarias
~ Colesterol + Días.con.síntomas + Edad,
data = dolor,
family = "binomial")
summary(log_mult)
dolor_varones = dolor[dolor$Sexo ==0,]
dolor_mujeres = dolor[dolor$Sexo ==1,]
log_mult_varones <- glm(formula =  Estrechamiento.arterias.coronarias
~ Colesterol + Días.con.síntomas + Edad,
data = dolor_varones,
family = "binomial")
summary(log_mult_varones)
log_mult_mujeres <- glm(formula =  Estrechamiento.arterias.coronarias
~ Colesterol + Días.con.síntomas + Edad,
data = dolor_mujeres,
family = "binomial")
summary(log_mult_mujeres)
europa = read.xlsx("Europa.xlsx")
colnames(europa)= c("Pais","Area","PBI","Infl","ExpVid","Militar","CrecPob","Desempleo")
europa_Sigma = cov(europa[2:8])
knitr::kable(round(europa_Sigma,2))
det(europa_Sigma)
europa_autovalores <- eigen(europa_Sigma)$values
europa_autovalores
max(europa_autovalores)
europa_autovectores <- eigen(europa_Sigma)$vectors
europa_l1 <- europa_autovalores[1]
europa_v1 <- europa_autovectores[, 1]
europa_Sigma %*% europa_v1
options(width = 80)
europa_PC <- prcomp(europa[2:8], scale = TRUE)
europa_PC$rotation
summary(europa_PC)
fviz_screeplot(europa_PC, barfill = "pink", barcolor = "pink", linecolor = "purple", addlabels = TRUE, ncp = 12) +
geom_abline(slope = 0, intercept = 10, color = "red", linetype = "dashed")
# Usando las dos cuatro componenes prncipales
fviz_cos2(europa_PC, choice = "var", axes = 1:2, fill = "pink", color = "violet") +
ggtitle("Calidad en la representación de las variables en las dimensiones 1-2") +
theme(plot.title = element_text(hjust = 0.5))
johnson = JohnsonJohnson
options(width = 80)
# Convertir los datos a serie temporal
jj_ts <- ts(JohnsonJohnson, start = c(1960, 1), frequency = 4)
# Graficar la serie temporal
plot(jj_ts, main = "Ventas Trimestrales de Johnson & Johnson",
ylab = "Ventas en Millones de Dólares", xlab = "Año")
# Descomposición aditiva
decomp_additive <- decompose(jj_ts, type = "additive")
plot(decomp_additive)
# Descomposición multiplicativa
decomp_multiplicative <- decompose(jj_ts, type = "multiplicative")
plot(decomp_multiplicative)
johnson_lamb <- BoxCox.lambda(jj_ts)
johnson_BC <- BoxCox(jj_ts, lambda = johnson_lamb)
johnson_sto <- autoplot(jj_ts, color = "royalblue", main =  "Serie de tiempo original", xlab = "", ylab = "") +
theme_hc() +
theme(plot.title = element_text(hjust = 0.5))
johnson_stt <- autoplot(johnson_BC, color = "purple", main =  "Serie de tiempo transformada", xlab = "", ylab = "") +
theme_hc() +
theme(plot.title = element_text(hjust = 0.5))
grid.arrange(johnson_sto, johnson_stt, ncol = 1)
JohnsonJohnson_sin_ultimos <- head(JohnsonJohnson, -8)
jj_2_ts <- ts(JohnsonJohnson_sin_ultimos, start = c(1960, 1), frequency = 4)
JohnsonJohnson_prueba <- window(JohnsonJohnson, start = c(1979,1))
adf.test(JohnsonJohnson_sin_ultimos,alternative = "stationary")
pp.test(JohnsonJohnson_sin_ultimos,alternative = "stationary")
JohnsonJohnson_sin_ultimos_d1 = diff(JohnsonJohnson_sin_ultimos, differences = 1)
adf.test(JohnsonJohnson_sin_ultimos_d1,alternative = "stationary")
pp.test(JohnsonJohnson_sin_ultimos_d1,alternative = "stationary")
# Predicciones del primer modelo ARIMA
predicciones1 <- predict(JohnsonJohnson_sin_ultimos_manualARIMA, n.ahead = 8)
# Predicciones del segundo modelo ARIMA
predicciones2 <- predict(JohnsonJohnson_sin_ultimos_autoARIMA, n.ahead = 8)
# Extraer las ganancias reales de los dos últimos años
ganancias_reales <- JohnsonJohnson_prueba[(length(JohnsonJohnson_prueba)-8):length(JohnsonJohnson_prueba)]
# Calcular el MAPE para cada modelo
MAPE1 <- mean(abs((ganancias_reales - predicciones1$pred)/ganancias_reales))*100
MAPE2 <- mean(abs((ganancias_reales - predicciones2$pred)/ganancias_reales))*100
# Calcular el AIC para cada modelo
AIC1 <- AIC(JohnsonJohnson_sin_ultimos_manualARIMA)
AIC2 <- AIC(JohnsonJohnson_sin_ultimos_autoARIMA)
# Mostrar los resultados
print("Modelo 1:")
print(paste("MAPE:", MAPE1))
print(paste("AIC:", AIC1))
cat(print("Modelo 2:"))
print(paste("MAPE:", MAPE2))
print(paste("AIC:", AIC2))
# Predicciones del primer modelo ARIMA
predicciones1 <- predict(JohnsonJohnson_sin_ultimos_manualARIMA, n.ahead = 8)
# Predicciones del segundo modelo ARIMA
predicciones2 <- predict(JohnsonJohnson_sin_ultimos_autoARIMA, n.ahead = 8)
# Extraer las ganancias reales de los dos últimos años
ganancias_reales <- JohnsonJohnson_prueba[(length(JohnsonJohnson_prueba)-8):length(JohnsonJohnson_prueba)]
# Calcular el MAPE para cada modelo
MAPE1 <- mean(abs((ganancias_reales - predicciones1$pred)/ganancias_reales))*100
MAPE2 <- mean(abs((ganancias_reales - predicciones2$pred)/ganancias_reales))*100
# Calcular el AIC para cada modelo
AIC1 <- AIC(JohnsonJohnson_sin_ultimos_manualARIMA)
AIC2 <- AIC(JohnsonJohnson_sin_ultimos_autoARIMA)
# Mostrar los resultados
print("Modelo 1:")
print(paste("MAPE:", MAPE1))
print(paste("AIC:", AIC1))
cat("Modelo 2:")
print(paste("MAPE:", MAPE2))
print(paste("AIC:", AIC2))
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/rpart/241_gridsearch_esqueleto.r")
# cargo los datos
dataset <- fread("Desktop/ITBA/Mineria de Datos/datasets/dataset_pequeno.csv")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/rpart/241_gridsearch_esqueleto.r")
archivo_salida
tb_grid_search
fwrite(
tb_grid_search,
file = archivo_salida,
sep = "\t"
)
fwrite(
tb_grid_search,
file = "salida.txt",
sep = "\t"
)
write.csv(tb_grid_search, file = "output.csv", row.names = TRUE)
View(tb_grid_search)
write.csv(tb_grid_search,"Desktop/output.csv", row.names = TRUE)
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
filename = paste(gsub(":", "-", Sys.time()),"_file.csv",sep="")
# cargo el dataset
dataset <- fread("train.csv",drop = (4))
dataset_test <- fread("test.csv",drop = 3)
dataset = dataset[,-8]
dataset_test = dataset[,-7]
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
filename = paste(gsub(":", "-", Sys.time()),"_file.csv",sep="")
# cargo el dataset
dataset <- fread("~/Desktop/Titanic/train.csv",drop = (4))
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = 3)
dataset = dataset[,-8]
dataset_test = dataset[,-7]
dtrain <- dataset # defino donde voy a entrenar
dapply <- dataset_test # defino donde voy a aplicar el modelo
dim(dataset)
dim(dataset_test)
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "Survived ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 10,
cp = -0.1, # esto significa no limitar la complejidad de los splits
minsplit = 100, # minima cantidad de registros para que se haga el split
minbucket = 50, # tamaño minimo de una hoja
maxdepth = 6
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
colnames(dtrain)
colnames(dapply)
View(dataset)
View(dataset_test)
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
filename = paste(gsub(":", "-", Sys.time()),"_file.csv",sep="")
# cargo el dataset
dataset <- fread("~/Desktop/Titanic/train.csv",drop = (4))
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = 3)
View(dataset)
View(dataset_test)
dataset = dataset[,-8]
dataset_test = dataset[,-7]
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = 3)
dataset_test = dataset[,-7]
source("~/Desktop/Titanic/101_PrimerModelo.R")
source("~/Desktop/Titanic/101_PrimerModelo.R")
# cargo el dataset
dataset <- fread("~/Desktop/Titanic/train.csv",drop = (4))
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = (3))
dataset = dataset[,-8]
dataset_test = dataset[,-7]
View(dataset)
View(dataset_test)
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = (3))
View(dataset_test)
source("~/Desktop/Titanic/101_PrimerModelo.R")
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
filename = paste(gsub(":", "-", Sys.time()),"_file.csv",sep="")
# cargo el dataset
dataset <- fread("~/Desktop/Titanic/train.csv",drop = (4))
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = (3))
dataset = dataset[,-8]
dataset_test = dataset_test[,-7]
dtrain <- dataset # defino donde voy a entrenar
dapply <- dataset_test # defino donde voy a aplicar el modelo
dim(dataset)
dim(dataset_test)
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "Survived ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 10,
cp = -0.1, # esto significa no limitar la complejidad de los splits
minsplit = 100, # minima cantidad de registros para que se haga el split
minbucket = 50, # tamaño minimo de una hoja
maxdepth = 6
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
View(dataset)
View(dataset)
View(dataset_test)
# cargo el dataset
dataset <- fread("~/Desktop/Titanic/train.csv",drop = (4))
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = (3))
View(dataset)
dataset = dataset[,-8]
dataset_test = dataset_test[,-7]
dataset = dataset[,-9]
dataset_test = dataset_test[,-8]
dtrain <- dataset # defino donde voy a entrenar
dapply <- dataset_test # defino donde voy a aplicar el modelo
dim(dataset)
dim(dataset_test)
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "Survived ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 10,
cp = -0.1, # esto significa no limitar la complejidad de los splits
minsplit = 100, # minima cantidad de registros para que se haga el split
minbucket = 50, # tamaño minimo de una hoja
maxdepth = 6
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, Survived := prediccion[, "Survived"]]
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
View(dataset)
View(dataset_test)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "class"
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply,
type = "prob"
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply
)
source("~/Desktop/Titanic/101_PrimerModelo.R")
source("~/Desktop/Titanic/101_PrimerModelo.R")
source("~/Desktop/Titanic/101_PrimerModelo.R")
# cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
filename = paste(gsub(":", "-", Sys.time()),"_file.csv",sep="")
# cargo el dataset
dataset <- fread("~/Desktop/Titanic/train.csv",drop = (4))
dataset_test <- fread("~/Desktop/Titanic/test.csv",drop = (3))
dataset = dataset[,-8]
dataset_test = dataset_test[,-7]
dataset = dataset[,-9]
dataset_test = dataset_test[,-8]
dtrain <- dataset # defino donde voy a entrenar
dapply <- dataset_test # defino donde voy a aplicar el modelo
dim(dataset)
dim(dataset_test)
# genero el modelo,  aqui se construye el arbol
# quiero predecir clase_ternaria a partir de el resto de las variables
modelo <- rpart(
formula = "Survived ~ .",
data = dtrain, # los datos donde voy a entrenar
xval = 10,
cp = -0.1, # esto significa no limitar la complejidad de los splits
minsplit = 100, # minima cantidad de registros para que se haga el split
minbucket = 50, # tamaño minimo de una hoja
maxdepth = 6
) # profundidad maxima del arbol
# grafico el arbol
prp(modelo,
extra = 101, digits = -5,
branch = 1, type = 4, varlen = 0, faclen = 0
)
# aplico el modelo a los datos nuevos
prediccion <- predict(
object = modelo,
newdata = dapply
)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, Survived := prediccion[, "Survived"]]
View(dataset)
View(dapply)
View(dtrain)
source("~/Desktop/Titanic/101_PrimerModelo.R")
View(modelo)
View(dapply)
View(dtrain)
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob := prediccion[, "prob"]]
source("~/Desktop/Titanic/101_PrimerModelo.R")
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, Survived := prediccion[, "1"]]
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, Survived := prediccion[, 1]]
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob := prediccion[, "Survived"]]
prediction[,"Survived"]
prediction[,"1"]
prediccion[,"Survived"]
prediccion[,"Fare"]
prediccion["2"]
prediccion["1"]
prediccion["3"]
prediccion["5"]
prediccion["6"]
# agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[, prob := prediccion[2]]
# solo le envio estimulo a los registros
#  con probabilidad de BAJA+2 mayor  a  1/40
dapply[, Predicted := as.numeric(Survived > 1 / 40)]
# genero el archivo para Kaggle
# primero creo la carpeta donde va el experimento
dir.create("./exp/")
dir.create("./exp/KA2001")
# solo los campos para Kaggle
fwrite(dapply[, list(PassengerId, Survived)],
file = filename,
sep = ","
)
View(dapply)
View(dtrain)
View(dtrain)
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
dt <- data.table(name = names(resultados_aciertos), value = unlist(resultados_aciertos))
dt2 <- data.table(name = names(resultados_tiros), value = unlist(resultados_tiros))
d3 = merge(dt, dt2, by = "name", all = FALSE)
write.csv(d3, "final3.csv")
setwd("Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/")
write.csv(d3, "final3.csv")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
dt <- data.table(name = names(resultados_aciertos), value = unlist(resultados_aciertos))
dt2 <- data.table(name = names(resultados_tiros), value = unlist(resultados_tiros))
d3 = merge(dt, dt2, by = "name", all = FALSE)
setwd("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/")
write.csv(d3, "final4.csv")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_B.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_02.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_02.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_02.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_01.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_02.r")
source("~/Desktop/ITBA/Mineria de Datos/dm2024a/src/CazaTalentos/zintento_A_02.r")
